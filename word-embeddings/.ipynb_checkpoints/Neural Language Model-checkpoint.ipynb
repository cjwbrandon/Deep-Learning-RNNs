{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Word Embeddings are a featurised representation of each word in our vocabulary. One of the main advantages of word embeddings is that it allow us to generalise the words well. With one-hot encoding, each word is treated individually without any information on how they relate with one another. For example, boy vs. girl, apple vs. orange, old vs. young, etc. In addition, using one-hot vectors quickly becomes impractical as we scale to larger and larger vocabulary sizes. Let say we have a 100 million vocab size and convert each word into a one-hot vector. Imagine the memory costs loading a 1 billion words long passage as well as the computation cost of calculating the softmax function on 1 million classes. As such, we require an alternative representation.\n",
    "\n",
    "What happens is that word embeddings attempts to solve these issues by mapping these words to a smaller set of features which allows the Network to generalise better especially with unseen words. Word embeddings aims to capitalise on the hidden semantic relationships of words to reduce the dimensions of a corpus of words. For example, if we have a vocab size of 10,000 words, we will attempt to reduce this to a featurised representation of lets say 300 different features. With this reduced representation, it will be forced to incorporate the main differentiating characteristics into these 300 feature set.\n",
    "\n",
    "In this series, we will focus on a couple of methodologies to training a Word Embedding namely using a Neural Language Model, Skip-Gram/ Word2Vec Model and GloVe Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "\n",
    "I will be using the WikiText-2 dataset to train a word embedding using a Neural Language Model. We can download the dataset using the Classes available in torchText.datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import PennTreebank\n",
    "from torchtext.data import Field\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.data import BPTTIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "prep_text = Field(lower=True, batch_first=True, tokenize=\"spacy\")\n",
    "# load dataset\n",
    "train, valid, test = PennTreebank.splits(prep_text, root=\"../../data/NLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab object\n",
    "prep_text.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create iterator\n",
    "seq_len = 30\n",
    "batch_size = 32\n",
    "\n",
    "train_iter, valid_iter, test_iter = BPTTIterator.splits([train, valid, test], batch_size=batch_size, bptt_len=seq_len, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample batch\n",
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'fields', 'text', 'target'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(batch).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   6, 9703, 9704, 9705, 9707, 9708, 9709, 9712, 9713, 9714, 9715,   23,\n",
      "         5160, 9716, 9718, 9719, 9720, 9721, 9723, 9724, 9725, 9726, 9727, 9728,\n",
      "           23,  508, 9729, 9730, 9731,    7],\n",
      "        [   4,   13,   30,   14,  762,  127,    2, 1683,  109,  298,   19,  108,\n",
      "         1218,   17,  304,    7,    6,   72,  293,  780, 2480,   10,    3,    5,\n",
      "            4,    2, 3228, 6058,    9, 1642],\n",
      "        [   4,  122, 7942,    7,    6, 2330,  946,    7,    6,   38,  177,   23,\n",
      "          891,  234,   19,  595, 4253, 1173,   92,   19, 2965, 1580,  195,    2,\n",
      "          205,   12,   42,   99,   13,  464],\n",
      "        [  21,    7,    6,    2,  515,  923,   67,   49,   32, 4557,   22,    2,\n",
      "         4466,  160,  986,   10, 2008,  676,   35,   21,    7,    6,  113,   49,\n",
      "           32,    3,    5,    4,   10, 3725],\n",
      "        [5364, 1149,    7,    6,   10,  456,    3,    5,    4,    3,    5,    4,\n",
      "            2,    3,    5,    4, 4986,    3,    5,    4,   52, 7950, 3303,    2,\n",
      "           42, 1094,   37,   11,    3,    5],\n",
      "        [4344,  323,   24, 2513,  711,   18,    8,   28,   12, 3423,   15,  788,\n",
      "          632,    7,    6,    2,  429,   19,   16,   83, 1812, 3221,   10,    3,\n",
      "            5,    4,    9, 1015,   10,  151],\n",
      "        [  66,  854,  122,  344,   35,   31, 1249,   22,   11,    3,    5,    4,\n",
      "         5697,    7,    6, 3259,   22,    2,   79,   72,   89,  721,   32,   11,\n",
      "            3,    5,    4,  367,   24,    2],\n",
      "        [  67, 4934,   11,  303, 7045,   12, 4489,    2, 3623,    3,    5,    4,\n",
      "         4489,  874,  325,    3,    5,    4,   12, 5160,  291,   13,    3,    5,\n",
      "            4, 8902,   13, 3795,    9,    3],\n",
      "        [   5,    4,   72, 6625,    3,    5,    4, 1908,  109,    3,    5,    4,\n",
      "           38, 3292,    7,    6,    2, 2920,  336, 2110,   31, 5544,   12,    2,\n",
      "         8772, 6370, 2841, 9549, 1548, 2945],\n",
      "        [   4,   13, 2337,   24, 9487,    3,    5,    4, 4069,  638,    3,    5,\n",
      "            4,   33,    3,    5,    4,    3,    5,    4,    9,  674,    7,    6,\n",
      "          785,   58,   30,   14, 6866,   66],\n",
      "        [ 945,  390,   16,   65,  118,   33,  647,   11,  342, 1503,    7,    6,\n",
      "           46, 1146,  181,   10,  392,   65,   69, 1869,   12,   59, 2562,   21,\n",
      "          821, 2080,    3,    5,    4,   84],\n",
      "        [   5,    4, 1839,   12,    3,    5,    4,  269,  593,   37, 1221, 1427,\n",
      "           98,    2,  110, 2133,  194,  126,   22, 2628,  972,   17,   11, 9049,\n",
      "           45,  130,   20, 1348,   29, 2170],\n",
      "        [  11,  895,  386,   16,   31,  275,   10, 3505, 2310,  195,    2,  169,\n",
      "           13,  441, 3756,    7,    6,  895, 3252,   33, 6676, 3952,   17,    3,\n",
      "            5,    4,    7,    6,   46,   33],\n",
      "        [   2,  303, 2168,  698,  418, 8076,  625,   58, 3993,   16,  283,  131,\n",
      "           12,    2,  982, 1209,   13,    2,    3,    5,    4,   12, 1491,  697,\n",
      "            9,    3,    5,    4, 3499,   33],\n",
      "        [ 142,   33,  230,  629,   10,    3,    5,    4,   11,  600,    9,  223,\n",
      "           12,  117,   17,  778,   11,    8,    8,  566,  335,   12, 1811,  652,\n",
      "            7,    6,   46, 1461, 7053,    2],\n",
      "        [  68,    7,    6,  468,   13, 1185,  358, 1583,  408,   33, 5418,    7,\n",
      "            6,    2, 4722, 1580, 2903, 1185,  183,   29,    2, 3761,  902,  269,\n",
      "           25,  312,  132,  179,   41,  263],\n",
      "        [ 289,   93,  359,   37,   58, 2872, 1418,   12, 2330,  196, 1980,  186,\n",
      "           13, 2391,  186,   12,   34, 1025, 2363,  101,  157,    7,    6, 5265,\n",
      "          101, 2800,   33, 4249,    3,    5],\n",
      "        [   8,   28,   27,    2,  369,  231,   10,   11, 1061,   25,    3,    5,\n",
      "            4,    3,    5,    4,    9,  742,  414,   14,  804, 3053,    7,    6,\n",
      "            2,   80,   40,   32,  424,   12],\n",
      "        [ 374, 1127, 4835,    9,   74, 1142,    3,    5,    4, 2623, 3300, 2777,\n",
      "           17,    2,  930,   24, 1182,  687, 4456,  334, 1129,   13,    2, 6991,\n",
      "         1193,    7,    6,    2,   74, 5485],\n",
      "        [1049,  949,    9,    2, 1942, 2372, 2528,    7,    6,   92,   54,  108,\n",
      "         1943,  676,    9, 2782,   36,  175,   21,   11,  313,    9,  449,   54,\n",
      "          160,    3,    5,    4,   17,   78],\n",
      "        [ 301,    7,    6,   76,  215, 2673,   13, 2464, 3181,   59,  224,   45,\n",
      "            3,    5,    4,  178, 1401,   26,   93,  626,    3,    5,    4,   13,\n",
      "            3,    5,    4,    7,    6,  111],\n",
      "        [1113,    9,  416,   34,  198,   12, 2518,  661,   36,  649,    2, 1500,\n",
      "           10,  508,  105, 9670,    3,    5,    4,    7,    6,   20,  504,   19,\n",
      "         1433, 1500,   17, 3882, 2408,  551],\n",
      "        [1068,   10,   32,  735,    2, 2338,    9,  196,    3,    5,    4, 6644,\n",
      "          238,   29, 1922,    9, 1455,   15, 5375,   13,  108,  353,   94,    2,\n",
      "         1828,  534,    3,    5,    4,  364],\n",
      "        [6706,   16,   15,   66,  277,   12,    2, 1445, 9018,   13,   16,   97,\n",
      "           11, 1473,   89,  201,  400,   12,    2,  143,  267,   85, 5181,    7,\n",
      "            6, 6222,  115,    3,    5,    4],\n",
      "        [6838, 1104,   12, 3856,   62,    7,    6,    2,  481,    9, 1346, 3316,\n",
      "         3480,   24,    2,  141,  156,  394,  131, 2328, 1928,   10,    8,   27,\n",
      "           78,  148,   15,    8,    7,    6],\n",
      "        [   7,    6, 4126,   15, 2083, 1177,   11,  256,   47,  135,    9, 1815,\n",
      "         1312,  147,   17,   34,   84,   88,  343,  641,   14,    8,    7,    6,\n",
      "            2,   62,   31,   85,   25,    8],\n",
      "        [   3,    5,    4,   44, 1126, 1246,  524, 1487,   16,  164,  401,  323,\n",
      "          298,    7,    6,  136,    2,  506,   11,   79,   49,   41,   11, 1380,\n",
      "         1992,  232,   43,   10,  519,   11],\n",
      "        [ 348, 6132,   55, 1415,   21,   13,  111,   72,  293,   81,   92,    3,\n",
      "            5,    4,   12,    2, 5002,    7,    6,   11,  279,  442,  184,   30,\n",
      "           14, 6132, 6570,  171,  302,   16],\n",
      "        [  11,  933,    9, 5055, 1617,  188,    2,  206,    7,    6, 6685,  299,\n",
      "           19,   11,  464,  677,    9, 6685,  146,  309,  414,   14,   11,  403,\n",
      "          146,  206,   16,   31,    3,    5],\n",
      "        [ 213,   93,  776,   70,   14,  583,   36, 1211,   10,  692,   42,  205,\n",
      "           75,   20,   19,   10, 1359,   34,  261,  145,    9,    8,    8,   13,\n",
      "           53,  232,   43,    7,    6,    2],\n",
      "        [   7,    6,  861,    3,    5,    4, 2164,  397,  111,   49,   32,    3,\n",
      "            5,    4,    7,    6,  134,   15,  853, 4255,  258,  355,  623,   13,\n",
      "           69,    3,    5,    4,  263, 1319],\n",
      "        [   5,    4,   18,    8,   12,   11,  114,  646,   12,    8,  130, 2201,\n",
      "          191,    9,   95,   18,    8,    7,    6, 1353,   15, 1377,    7,    6,\n",
      "            2, 1746,  558,   27,  693, 1800]], device='cuda:0')\n",
      "torch.Size([32, 30])\n",
      "tensor([[9703, 9704, 9705, 9707, 9708, 9709, 9712, 9713, 9714, 9715,   23, 5160,\n",
      "         9716, 9718, 9719, 9720, 9721, 9723, 9724, 9725, 9726, 9727, 9728,   23,\n",
      "          508, 9729, 9730, 9731,    7,    6],\n",
      "        [  13,   30,   14,  762,  127,    2, 1683,  109,  298,   19,  108, 1218,\n",
      "           17,  304,    7,    6,   72,  293,  780, 2480,   10,    3,    5,    4,\n",
      "            2, 3228, 6058,    9, 1642,    2],\n",
      "        [ 122, 7942,    7,    6, 2330,  946,    7,    6,   38,  177,   23,  891,\n",
      "          234,   19,  595, 4253, 1173,   92,   19, 2965, 1580,  195,    2,  205,\n",
      "           12,   42,   99,   13,  464,   10],\n",
      "        [   7,    6,    2,  515,  923,   67,   49,   32, 4557,   22,    2, 4466,\n",
      "          160,  986,   10, 2008,  676,   35,   21,    7,    6,  113,   49,   32,\n",
      "            3,    5,    4,   10, 3725,   12],\n",
      "        [1149,    7,    6,   10,  456,    3,    5,    4,    3,    5,    4,    2,\n",
      "            3,    5,    4, 4986,    3,    5,    4,   52, 7950, 3303,    2,   42,\n",
      "         1094,   37,   11,    3,    5,    4],\n",
      "        [ 323,   24, 2513,  711,   18,    8,   28,   12, 3423,   15,  788,  632,\n",
      "            7,    6,    2,  429,   19,   16,   83, 1812, 3221,   10,    3,    5,\n",
      "            4,    9, 1015,   10,  151, 1531],\n",
      "        [ 854,  122,  344,   35,   31, 1249,   22,   11,    3,    5,    4, 5697,\n",
      "            7,    6, 3259,   22,    2,   79,   72,   89,  721,   32,   11,    3,\n",
      "            5,    4,  367,   24,    2,  227],\n",
      "        [4934,   11,  303, 7045,   12, 4489,    2, 3623,    3,    5,    4, 4489,\n",
      "          874,  325,    3,    5,    4,   12, 5160,  291,   13,    3,    5,    4,\n",
      "         8902,   13, 3795,    9,    3,    5],\n",
      "        [   4,   72, 6625,    3,    5,    4, 1908,  109,    3,    5,    4,   38,\n",
      "         3292,    7,    6,    2, 2920,  336, 2110,   31, 5544,   12,    2, 8772,\n",
      "         6370, 2841, 9549, 1548, 2945,    7],\n",
      "        [  13, 2337,   24, 9487,    3,    5,    4, 4069,  638,    3,    5,    4,\n",
      "           33,    3,    5,    4,    3,    5,    4,    9,  674,    7,    6,  785,\n",
      "           58,   30,   14, 6866,   66,  275],\n",
      "        [ 390,   16,   65,  118,   33,  647,   11,  342, 1503,    7,    6,   46,\n",
      "         1146,  181,   10,  392,   65,   69, 1869,   12,   59, 2562,   21,  821,\n",
      "         2080,    3,    5,    4,   84,  251],\n",
      "        [   4, 1839,   12,    3,    5,    4,  269,  593,   37, 1221, 1427,   98,\n",
      "            2,  110, 2133,  194,  126,   22, 2628,  972,   17,   11, 9049,   45,\n",
      "          130,   20, 1348,   29, 2170,  250],\n",
      "        [ 895,  386,   16,   31,  275,   10, 3505, 2310,  195,    2,  169,   13,\n",
      "          441, 3756,    7,    6,  895, 3252,   33, 6676, 3952,   17,    3,    5,\n",
      "            4,    7,    6,   46,   33,  531],\n",
      "        [ 303, 2168,  698,  418, 8076,  625,   58, 3993,   16,  283,  131,   12,\n",
      "            2,  982, 1209,   13,    2,    3,    5,    4,   12, 1491,  697,    9,\n",
      "            3,    5,    4, 3499,   33, 6598],\n",
      "        [  33,  230,  629,   10,    3,    5,    4,   11,  600,    9,  223,   12,\n",
      "          117,   17,  778,   11,    8,    8,  566,  335,   12, 1811,  652,    7,\n",
      "            6,   46, 1461, 7053,    2,  506],\n",
      "        [   7,    6,  468,   13, 1185,  358, 1583,  408,   33, 5418,    7,    6,\n",
      "            2, 4722, 1580, 2903, 1185,  183,   29,    2, 3761,  902,  269,   25,\n",
      "          312,  132,  179,   41,  263, 2315],\n",
      "        [  93,  359,   37,   58, 2872, 1418,   12, 2330,  196, 1980,  186,   13,\n",
      "         2391,  186,   12,   34, 1025, 2363,  101,  157,    7,    6, 5265,  101,\n",
      "         2800,   33, 4249,    3,    5,    4],\n",
      "        [  28,   27,    2,  369,  231,   10,   11, 1061,   25,    3,    5,    4,\n",
      "            3,    5,    4,    9,  742,  414,   14,  804, 3053,    7,    6,    2,\n",
      "           80,   40,   32,  424,   12,  282],\n",
      "        [1127, 4835,    9,   74, 1142,    3,    5,    4, 2623, 3300, 2777,   17,\n",
      "            2,  930,   24, 1182,  687, 4456,  334, 1129,   13,    2, 6991, 1193,\n",
      "            7,    6,    2,   74, 5485,   48],\n",
      "        [ 949,    9,    2, 1942, 2372, 2528,    7,    6,   92,   54,  108, 1943,\n",
      "          676,    9, 2782,   36,  175,   21,   11,  313,    9,  449,   54,  160,\n",
      "            3,    5,    4,   17,   78, 1082],\n",
      "        [   7,    6,   76,  215, 2673,   13, 2464, 3181,   59,  224,   45,    3,\n",
      "            5,    4,  178, 1401,   26,   93,  626,    3,    5,    4,   13,    3,\n",
      "            5,    4,    7,    6,  111,    3],\n",
      "        [   9,  416,   34,  198,   12, 2518,  661,   36,  649,    2, 1500,   10,\n",
      "          508,  105, 9670,    3,    5,    4,    7,    6,   20,  504,   19, 1433,\n",
      "         1500,   17, 3882, 2408,  551,    7],\n",
      "        [  10,   32,  735,    2, 2338,    9,  196,    3,    5,    4, 6644,  238,\n",
      "           29, 1922,    9, 1455,   15, 5375,   13,  108,  353,   94,    2, 1828,\n",
      "          534,    3,    5,    4,  364, 5611],\n",
      "        [  16,   15,   66,  277,   12,    2, 1445, 9018,   13,   16,   97,   11,\n",
      "         1473,   89,  201,  400,   12,    2,  143,  267,   85, 5181,    7,    6,\n",
      "         6222,  115,    3,    5,    4,  325],\n",
      "        [1104,   12, 3856,   62,    7,    6,    2,  481,    9, 1346, 3316, 3480,\n",
      "           24,    2,  141,  156,  394,  131, 2328, 1928,   10,    8,   27,   78,\n",
      "          148,   15,    8,    7,    6,    2],\n",
      "        [   6, 4126,   15, 2083, 1177,   11,  256,   47,  135,    9, 1815, 1312,\n",
      "          147,   17,   34,   84,   88,  343,  641,   14,    8,    7,    6,    2,\n",
      "           62,   31,   85,   25,    8,  164],\n",
      "        [   5,    4,   44, 1126, 1246,  524, 1487,   16,  164,  401,  323,  298,\n",
      "            7,    6,  136,    2,  506,   11,   79,   49,   41,   11, 1380, 1992,\n",
      "          232,   43,   10,  519,   11, 1200],\n",
      "        [6132,   55, 1415,   21,   13,  111,   72,  293,   81,   92,    3,    5,\n",
      "            4,   12,    2, 5002,    7,    6,   11,  279,  442,  184,   30,   14,\n",
      "         6132, 6570,  171,  302,   16,   20],\n",
      "        [ 933,    9, 5055, 1617,  188,    2,  206,    7,    6, 6685,  299,   19,\n",
      "           11,  464,  677,    9, 6685,  146,  309,  414,   14,   11,  403,  146,\n",
      "          206,   16,   31,    3,    5,    4],\n",
      "        [  93,  776,   70,   14,  583,   36, 1211,   10,  692,   42,  205,   75,\n",
      "           20,   19,   10, 1359,   34,  261,  145,    9,    8,    8,   13,   53,\n",
      "          232,   43,    7,    6,    2,   51],\n",
      "        [   6,  861,    3,    5,    4, 2164,  397,  111,   49,   32,    3,    5,\n",
      "            4,    7,    6,  134,   15,  853, 4255,  258,  355,  623,   13,   69,\n",
      "            3,    5,    4,  263, 1319,    2],\n",
      "        [   4,   18,    8,   12,   11,  114,  646,   12,    8,  130, 2201,  191,\n",
      "            9,   95,   18,    8,    7,    6, 1353,   15, 1377,    7,    6,    2,\n",
      "         1746,  558,   27,  693, 1800,   12]], device='cuda:0')\n",
      "torch.Size([32, 30])\n"
     ]
    }
   ],
   "source": [
    "print(batch.text)\n",
    "print(batch.text.shape)\n",
    "print(batch.target)\n",
    "print(batch.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, n_embedding, n_hidden, n_layers, drop_prob=0):\n",
    "        super(Lang_Model, self).__init__()\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.emb = nn.Embedding(vocab_size, n_embedding, sparse=True)\n",
    "        \n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(n_embedding, n_hidden, n_layers, batch_first=True, nonlinearity='relu')\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # Linear layer\n",
    "        self.output = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "        # HPs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "    def forward(self, x, hidden): # RNN have an additional input, hidden. This refers to a[0], the initial activation to be fed into the first cell\n",
    "        \n",
    "        # Convert one-hot to embeddings\n",
    "        output = self.dropout(self.emb(x))\n",
    "        \n",
    "        # RNN\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        \n",
    "        # dropout layer\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # reshape\n",
    "        output = output.contiguous().view(-1, n_hidden)\n",
    "        \n",
    "        # output layer\n",
    "        output = self.output(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, cuda): # Initialise our hidden units, a[0]\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        # initialise weights to have dimensions layers x batch size x hidden units\n",
    "        if cuda:\n",
    "            hidden = torch.zeros(self.n_layers, batch_size, self.n_hidden, dtype=torch.float, requires_grad=True, device='cuda')\n",
    "            #hidden = weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda()\n",
    "        else:\n",
    "            hidden = torch.zeros(self.n_layers, batch_size, self.n_hidden, dtype=torch.float, requires_grad=True, device='cpu')\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for gpu\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lang_Model(\n",
      "  (emb): Embedding(9732, 50, sparse=True)\n",
      "  (rnn): RNN(50, 32, batch_first=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output): Linear(in_features=32, out_features=9732, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# HPs\n",
    "vocab_size = len(prep_text.vocab.itos)\n",
    "n_embedding = 50\n",
    "n_hidden = 32\n",
    "n_layers = 1\n",
    "drop_prob = 0.2\n",
    "\n",
    "# initialise model\n",
    "lang_model = Lang_Model(vocab_size, n_embedding, n_hidden, n_layers, drop_prob)\n",
    "print(lang_model)\n",
    "\n",
    "if cuda:\n",
    "    lang_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(lang_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.74 GiB (GPU 0; 7.79 GiB total capacity; 5.45 GiB already allocated; 1.46 GiB free; 5.47 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-49ee3115e450>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# forwrard prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mhid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9faf5811d2db>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Convert one-hot to embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 0; 7.79 GiB total capacity; 5.45 GiB already allocated; 1.46 GiB free; 5.47 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "clip = 5 # clip gradients to avoid exploding gradients\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(e)\n",
    "    \n",
    "    # training set\n",
    "    # initialise hidden state\n",
    "    hid = lang_model.init_hidden(batch_size, cuda)\n",
    "    running_train_loss = []\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        \n",
    "        # convert to one-hot encoders\n",
    "        text = F.one_hot(batch.text, vocab_size)\n",
    "        #targets = batch.target # using NLLLoss\n",
    "        targets = F.one_hot(batch.target, vocab_size)\n",
    "            \n",
    "        # zero gradients\n",
    "        lang_model.zero_grad()\n",
    "        \n",
    "        # forwrard prop\n",
    "        output, hid = lang_model(text, hid)\n",
    "        hid = hid.data\n",
    "        \n",
    "        # calculate loss\n",
    "        #m = nn.LogSoftmax(dim=1) # using NLLLoss\n",
    "        loss = criterion(output, torch.max(targets.view(batch_size*seq_len, -1), 1)[1])\n",
    "        running_train_loss.append(loss.item())\n",
    "        loss.backward() # backprop\n",
    "        \n",
    "        # clip gradient\n",
    "        nn.utils.clip_grad_norm_(lang_model.parameters(), clip)\n",
    "        optimiser.step()\n",
    "        \n",
    "    train_loss.append(np.mean(running_train_loss))\n",
    "    \n",
    "    # val set\n",
    "    # initialise hidden state\n",
    "    hid = lang_model.init_hidden(batch_size, cuda)\n",
    "    running_val_loss = []\n",
    "\n",
    "    for batch in valid_iter:\n",
    "        \n",
    "        # convert to one-hot encoders\n",
    "        text = F.one_hot(batch.text, vocab_size)\n",
    "        targets = batch.target\n",
    "        #targets = F.one_hot(batch.target, vocab_size)\n",
    "        \n",
    "        # forwrard prop\n",
    "        output, hid = lang_model(text, hid)\n",
    "        hid = hid.data\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(output, torch.max(targets.view(batch_size*seq_len, -1), 1)[1])\n",
    "        running_val_loss.append(loss.item())\n",
    "        \n",
    "    val_loss.append(np.mean(running_val_loss))\n",
    "    \n",
    "    print(f\"Epochs: {int(e+1)}/{int(epochs)} --- Train loss: {np.mean(running_train_loss)} --- Val loss: {np.mean(running_val_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
